{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cafdaad-563d-4805-a31d-eca76fc5dda6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13b0767d-68e0-4233-afaa-18bc6ca07da2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e270664-f31e-441a-891d-b6bbab08177b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[1] Loading COVID-19 Dataset...\nDataset loaded in 0.04 seconds\nInitial row count: 1,227,256\nColumns: ['date', 'county', 'state', 'fips', 'cases', 'deaths']\n"
     ]
    }
   ],
   "source": [
    "# Load the COVID-19 dataset\n",
    "print(\"\\n[1] Loading COVID-19 Dataset...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Load without schema inference to avoid type issues\n",
    "df_covid = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .csv(\"/databricks-datasets/COVID/covid-19-data/\")\n",
    "\n",
    "# Clean and cast data types using expr with try_cast (handles malformed data gracefully)\n",
    "df_covid = df_covid \\\n",
    "    .withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"cases\", coalesce(expr(\"try_cast(cases as int)\"), lit(0))) \\\n",
    "    .withColumn(\"deaths\", coalesce(expr(\"try_cast(deaths as int)\"), lit(0)))\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"Dataset loaded in {load_time:.2f} seconds\")\n",
    "print(f\"Initial row count: {df_covid.count():,}\")\n",
    "print(f\"Columns: {df_covid.columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9032d543-fb00-4732-a505-a435815eef41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size: 2,567,706,254 bytes\nTotal size: 2448.76 MB\nTotal size: 2.39 GB\n"
     ]
    }
   ],
   "source": [
    "# Test to see if the data is available\n",
    "\n",
    "def get_directory_size(path):\n",
    "    total_size = 0\n",
    "    try:\n",
    "        files = dbutils.fs.ls(path)\n",
    "        for file in files:\n",
    "            if file.isDir():\n",
    "                total_size += get_directory_size(file.path)\n",
    "            else:\n",
    "                total_size += file.size\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    return total_size\n",
    "\n",
    "path = \"/databricks-datasets/COVID/covid-19-data/\"\n",
    "size_bytes = get_directory_size(path)\n",
    "size_mb = size_bytes / (1024 * 1024)\n",
    "size_gb = size_bytes / (1024 * 1024 * 1024)\n",
    "\n",
    "print(f\"Total size: {size_bytes:,} bytes\")\n",
    "print(f\"Total size: {size_mb:.2f} MB\")\n",
    "print(f\"Total size: {size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d218fe9-8135-474f-9f68-1374d04e9602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nDataset Schema:\nroot\n |-- date: date (nullable = true)\n |-- county: string (nullable = true)\n |-- state: string (nullable = true)\n |-- fips: string (nullable = true)\n |-- cases: integer (nullable = false)\n |-- deaths: integer (nullable = false)\n\n\nSample Data:\n+----------+---------+----------+-----+-----+------+\n|date      |county   |state     |fips |cases|deaths|\n+----------+---------+----------+-----+-----+------+\n|2020-01-21|Snohomish|Washington|53061|1    |0     |\n|2020-01-22|Snohomish|Washington|53061|1    |0     |\n|2020-01-23|Snohomish|Washington|53061|1    |0     |\n|2020-01-24|Cook     |Illinois  |17031|1    |0     |\n|2020-01-24|Snohomish|Washington|53061|1    |0     |\n+----------+---------+----------+-----+-----+------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Check the data\n",
    "\n",
    "# Show schema\n",
    "print(\"\\nDataset Schema:\")\n",
    "df_covid.printSchema()\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nSample Data:\")\n",
    "df_covid.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07798e11-095c-42fb-aafb-d40b9c1a53e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Apply transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "402df5a3-31bd-4279-a435-4ab3822d886b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date columns found: ['date']\nAfter filtering: 1,224,611 rows\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORMATION 1: Filter Operations (Applied Early for Optimization)\n",
    "\n",
    "# Filter 1: Select data from 2021 onwards\n",
    "date_cols = [col for col in df_covid.columns if 'date' in col.lower()]\n",
    "print(f\"Date columns found: {date_cols}\")\n",
    "\n",
    "# Filter 2: Remove null values in key columns\n",
    "df_covid_valid = df_covid.filter(\n",
    "    expr(\"try_to_date(date, 'yyyy-MM-dd') IS NOT NULL\")\n",
    ")\n",
    "\n",
    "df_filtered = df_covid_valid.filter(col(\"cases\") > 0)\n",
    "\n",
    "print(f\"After filtering: {df_filtered.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72ccf51b-b655-4b20-98e3-094a71035a11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New columns added: year, month, quarter, cases_per_death, is_high_cases\n+----------+---------+----------+-----+-----+------+----+-----+-------+---------------+-------------+\n|      date|   county|     state| fips|cases|deaths|year|month|quarter|cases_per_death|is_high_cases|\n+----------+---------+----------+-----+-----+------+----+-----+-------+---------------+-------------+\n|2020-01-21|Snohomish|Washington|53061|    1|     0|2020|    1|      1|           NULL|          Low|\n|2020-01-22|Snohomish|Washington|53061|    1|     0|2020|    1|      1|           NULL|          Low|\n|2020-01-23|Snohomish|Washington|53061|    1|     0|2020|    1|      1|           NULL|          Low|\n|2020-01-24|     Cook|  Illinois|17031|    1|     0|2020|    1|      1|           NULL|          Low|\n|2020-01-24|Snohomish|Washington|53061|    1|     0|2020|    1|      1|           NULL|          Low|\n+----------+---------+----------+-----+-----+------+----+-----+-------+---------------+-------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORMATION 2: Column Transformations using withColumn\n",
    "\n",
    "df_transformed = df_filtered \\\n",
    "    .withColumn(\"year\", year(col(\"date\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"date\"))) \\\n",
    "    .withColumn(\"quarter\", quarter(col(\"date\"))) \\\n",
    "    .withColumn(\"cases_per_death\", \n",
    "                when((col(\"deaths\") > 0) & (col(\"deaths\").isNotNull()), \n",
    "                     col(\"cases\").cast(\"double\") / col(\"deaths\").cast(\"double\"))\n",
    "                .otherwise(lit(None).cast(\"double\"))) \\\n",
    "    .withColumn(\"is_high_cases\", when(col(\"cases\") > 10000, \"High\").otherwise(\"Low\"))\n",
    "\n",
    "print(\"New columns added: year, month, quarter, cases_per_death, is_high_cases\")\n",
    "df_transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d93c5b75-ae85-484e-9dc7-f99951e7387c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly Statistics Aggregated\n+--------------+----+-----+-----------+------------+------------------+---------------+-----------+--------------+\n|         state|year|month|total_cases|total_deaths|   avg_daily_cases|max_daily_cases|num_records|mortality_rate|\n+--------------+----+-----+-----------+------------+------------------+---------------+-----------+--------------+\n|          Ohio|2020|    5|     847509|       49709| 310.5566141443752|           5862|       2729|          5.87|\n|      Nebraska|2020|    5|     305559|        3838|145.78196564885496|           4314|       2096|          1.26|\n| West Virginia|2020|    3|        866|           3| 4.224390243902439|             31|        205|          0.35|\n|       Montana|2020|    3|       1200|          11| 5.555555555555555|             74|        216|          0.92|\n|        Hawaii|2020|    5|      19430|         525|156.69354838709677|            421|        124|          2.70|\n|      Nebraska|2020|    3|       1682|          13| 7.715596330275229|            113|        218|          0.77|\n|        Oregon|2020|    3|       4621|         129|11.971502590673575|            186|        386|          2.79|\n|South Carolina|2020|    5|     269994|       11736| 189.3366058906031|           1619|       1426|          4.35|\n|         Texas|2020|    2|         91|           0| 5.055555555555555|             11|         18|          0.00|\n| West Virginia|2020|    5|      47442|        1951|28.981062919975564|            308|       1637|          4.11|\n+--------------+----+-----+-----------+------------+------------------+---------------+-----------+--------------+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORMATION 3: GroupBy with Aggregations\n",
    "\n",
    "# Aggregation 1: Monthly statistics by state/region\n",
    "df_monthly_stats = df_transformed \\\n",
    "    .groupBy(\"state\", \"year\", \"month\") \\\n",
    "    .agg(\n",
    "        sum(\"cases\").alias(\"total_cases\"),\n",
    "        sum(\"deaths\").alias(\"total_deaths\"),\n",
    "        avg(\"cases\").alias(\"avg_daily_cases\"),\n",
    "        max(\"cases\").alias(\"max_daily_cases\"),\n",
    "        count(\"*\").alias(\"num_records\")\n",
    "    ) \\\n",
    "    .withColumn(\"mortality_rate\", \n",
    "                (col(\"total_deaths\") / col(\"total_cases\") * 100).cast(\"decimal(10,2)\"))\n",
    "\n",
    "print(\"Monthly Statistics Aggregated\")\n",
    "df_monthly_stats.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d489fb4c-e6e5-4b7b-92a9-aac8535ee9e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Yearly Statistics:\n+-----+----+------------+-------------+------------------+\n|state|year|yearly_cases|yearly_deaths| avg_cases_per_day|\n+-----+----+------------+-------------+------------------+\n|   01|2020|      526388|            0|1866.6241134751774|\n|   01|2021|      552952|            0| 7899.314285714286|\n|   02|2020|       13147|            0| 46.95357142857143|\n|   02|2021|       17911|            0|255.87142857142857|\n|   04|2020|     1048821|            0|3654.4285714285716|\n|   04|2021|      937029|            0|13386.128571428571|\n|   05|2020|      285868|            0|1010.1342756183745|\n|   05|2021|      337571|            0| 4822.442857142857|\n|   06|2020|     3065113|            0|10115.884488448844|\n|   06|2021|     2940139|            0|42001.985714285714|\n+-----+----+------------+-------------+------------------+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Aggregation 2: Year-over-year comparison\n",
    "df_yearly = df_transformed \\\n",
    "    .groupBy(\"state\", \"year\") \\\n",
    "    .agg(\n",
    "        sum(\"cases\").alias(\"yearly_cases\"),\n",
    "        sum(\"deaths\").alias(\"yearly_deaths\"),\n",
    "        avg(\"cases\").alias(\"avg_cases_per_day\")\n",
    "    ) \\\n",
    "    .orderBy(\"state\", \"year\")\n",
    "\n",
    "print(\"\\n Yearly Statistics:\")\n",
    "df_yearly.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9868790b-25e8-4114-8fb8-6c3d55aab92a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[5] Applying Window Functions...\nTop 10 states by cases per year:\n+----------+----+-----+-----------+-------------+\n|     state|year|month|total_cases|rank_by_cases|\n+----------+----+-----+-----------+-------------+\n|California|2020|   12|   53958843|            1|\n|     Texas|2020|   12|   47238530|            2|\n|   Florida|2020|   12|   35950671|            3|\n|     Texas|2020|   11|   33105809|            4|\n|California|2020|   11|   31809169|            5|\n|California|2020|   10|   27163983|            6|\n|   Florida|2020|   11|   26828632|            7|\n|  Illinois|2020|   12|   26821457|            8|\n|     Texas|2020|   10|   26765048|            9|\n|  New York|2020|   12|   25200181|           10|\n|California|2021|    2|  160698825|            1|\n|     Texas|2021|    2|  118293231|            2|\n|California|2021|    1|   90315276|            3|\n|   Florida|2021|    2|   84572548|            4|\n|California|2021|    3|   79088430|            5|\n|  New York|2021|    2|   71535329|            6|\n|     Texas|2021|    1|   64910955|            7|\n|     Texas|2021|    3|   59202086|            8|\n|  Illinois|2021|    2|   53725198|            9|\n|   Florida|2021|    1|   47886474|           10|\n+----------+----+-----+-----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORMATION 4: Window Functions for Ranking\n",
    "print(\"\\n[5] Applying Window Functions...\")\n",
    "\n",
    "window_spec = Window.partitionBy(\"year\").orderBy(desc(\"total_cases\"))\n",
    "\n",
    "df_ranked = df_monthly_stats \\\n",
    "    .withColumn(\"rank_by_cases\", rank().over(window_spec)) \\\n",
    "    .filter(col(\"rank_by_cases\") <= 10)\n",
    "\n",
    "print(\"Top 10 states by cases per year:\")\n",
    "df_ranked.select(\"state\", \"year\", \"month\", \"total_cases\", \"rank_by_cases\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26ed9da4-99a7-465c-a7c1-5e4f330c0148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d8638e8-62d6-4029-bca6-17e2957f2333",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[6] SQL Query 1: Top 10 States by Total Cases\n+--------------+-----------+------------+------------------+\n|         state|total_cases|total_deaths|mortality_rate_pct|\n+--------------+-----------+------------+------------------+\n|    California|  505068688|     7480314|              1.48|\n|         Texas|  402592904|     6841453|               1.7|\n|       Florida|  313235465|     5413037|              1.73|\n|      New York|  272678258|    12719543|              4.66|\n|      Illinois|  195082889|     4340764|              2.23|\n|       Georgia|  151749782|     2835421|              1.87|\n|          Ohio|  134921770|     2465310|              1.83|\n|  Pennsylvania|  133343845|     4195967|              3.15|\n|    New Jersey|  128802146|     6004001|              4.66|\n|North Carolina|  124271360|     1725065|              1.39|\n+--------------+-----------+------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Register DataFrame as temporary view\n",
    "df_transformed.createOrReplaceTempView(\"covid_data\")\n",
    "df_monthly_stats.createOrReplaceTempView(\"monthly_stats\")\n",
    "\n",
    "# SQL Query 1: Top 10 states with highest total cases\n",
    "print(\"\\n[6] SQL Query 1: Top 10 States by Total Cases\")\n",
    "query1 = \"\"\"\n",
    "    SELECT \n",
    "        state,\n",
    "        SUM(cases) as total_cases,\n",
    "        SUM(deaths) as total_deaths,\n",
    "        ROUND(SUM(deaths) / SUM(cases) * 100, 2) as mortality_rate_pct\n",
    "    FROM covid_data\n",
    "    WHERE state IS NOT NULL\n",
    "    GROUP BY state\n",
    "    ORDER BY total_cases DESC\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "df_sql1 = spark.sql(query1)\n",
    "df_sql1.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b82fb821-2b5e-4b17-a860-67c60989b692",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[7] SQL Query 2: Monthly Trend Analysis for 2021\n+-----+----------+-------------------+------------------+----------+\n|month|num_states|monthly_total_cases|avg_mortality_rate|peak_cases|\n+-----+----------+-------------------+------------------+----------+\n|    1|       110|          742182208|          0.798273|   1117346|\n|    2|       110|         1291966232|          0.825818|   1192559|\n|    3|       110|          643622995|          0.838000|   1208672|\n+-----+----------+-------------------+------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# SQL Query 2: Monthly trend analysis\n",
    "print(\"\\n[7] SQL Query 2: Monthly Trend Analysis for 2021\")\n",
    "query2 = \"\"\"\n",
    "    SELECT \n",
    "        month,\n",
    "        COUNT(DISTINCT state) as num_states,\n",
    "        SUM(total_cases) as monthly_total_cases,\n",
    "        AVG(mortality_rate) as avg_mortality_rate,\n",
    "        MAX(max_daily_cases) as peak_cases\n",
    "    FROM monthly_stats\n",
    "    WHERE year = 2021\n",
    "    GROUP BY month\n",
    "    ORDER BY month\n",
    "\"\"\"\n",
    "\n",
    "df_sql2 = spark.sql(query2)\n",
    "df_sql2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b3f5244-85d9-4d82-9b31-7b42ae23b020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## OPTIMIZATION STRATEGIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c6add6a-55e0-4eb9-a2db-c17c36fbe06c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\nProject [state#26658, year#26774, month#26776, total_cases#27428L, total_deaths#27429L, avg_daily_cases#27430, max_daily_cases#27431, num_records#27432L, cast(((cast(total_deaths#27429L as double) / cast(total_cases#27428L as double)) * cast(100 as double)) as decimal(10,2)) AS mortality_rate#27439]\n+- Aggregate [state#26658, year#26774, month#26776], [state#26658, year#26774, month#26776, sum(cases#26770) AS total_cases#27428L, sum(deaths#26772) AS total_deaths#27429L, avg(cases#26770) AS avg_daily_cases#27430, max(cases#26770) AS max_daily_cases#27431, count(1) AS num_records#27432L]\n   +- Project [date#26768, county#26657, state#26658, fips#26659, cases#26770, deaths#26772, year#26774, month#26776, quarter#26778, cases_per_death#26780, CASE WHEN (cases#26770 > 10000) THEN High ELSE Low END AS is_high_cases#26782]\n      +- Project [date#26768, county#26657, state#26658, fips#26659, cases#26770, deaths#26772, year#26774, month#26776, quarter#26778, CASE WHEN ((deaths#26772 > 0) AND isnotnull(deaths#26772)) THEN (cast(cases#26770 as double) / cast(deaths#26772 as double)) ELSE cast(null as double) END AS cases_per_death#26780]\n         +- Project [date#26768, county#26657, state#26658, fips#26659, cases#26770, deaths#26772, year#26774, month#26776, quarter(date#26768) AS quarter#26778]\n            +- Project [date#26768, county#26657, state#26658, fips#26659, cases#26770, deaths#26772, year#26774, month(date#26768) AS month#26776]\n               +- Project [date#26768, county#26657, state#26658, fips#26659, cases#26770, deaths#26772, year(date#26768) AS year#26774]\n                  +- Filter (cases#26770 > 0)\n                     +- Filter isnotnull(try_to_date(date#26768, Some(yyyy-MM-dd), Some(Etc/UTC), false))\n                        +- Project [date#26768, county#26657, state#26658, fips#26659, cases#26770, coalesce(try_cast(deaths#26661 as int), 0) AS deaths#26772]\n                           +- Project [date#26768, county#26657, state#26658, fips#26659, coalesce(try_cast(cases#26660 as int), 0) AS cases#26770, deaths#26661]\n                              +- Project [to_date(date#26656, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS date#26768, county#26657, state#26658, fips#26659, cases#26660, deaths#26661]\n                                 +- Relation [date#26656,county#26657,state#26658,fips#26659,cases#26660,deaths#26661] csv\n\n== Analyzed Logical Plan ==\nstate: string, year: int, month: int, total_cases: bigint, total_deaths: bigint, avg_daily_cases: double, max_daily_cases: int, num_records: bigint, mortality_rate: decimal(10,2)\nProject [state#26658, year#26774, month#26776, total_cases#27428L, total_deaths#27429L, avg_daily_cases#27430, max_daily_cases#27431, num_records#27432L, cast(((cast(total_deaths#27429L as double) / cast(total_cases#27428L as double)) * cast(100 as double)) as decimal(10,2)) AS mortality_rate#27439]\n+- Aggregate [state#26658, year#26774, month#26776], [state#26658, year#26774, month#26776, sum(cases#26770) AS total_cases#27428L, sum(deaths#26772) AS total_deaths#27429L, avg(cases#26770) AS avg_daily_cases#27430, max(cases#26770) AS max_daily_cases#27431, count(1) AS num_records#27432L]\n   +- Project [date#26768, county#26657, state#26658, fips#26659, cases#26770, deaths#26772, year#26774, month#26776, quarter#26778, cases_per_death#26780, CASE WHEN (cases#26770 > 10000) THEN High ELSE Low END AS is_high_cases#26782]\n      +- Project [date#26768, county#26657, state#26658, fips#26659, cases#26770, deaths#26772, year#26774, month#26776, quarter#26778, CASE WHEN ((deaths#26772 > 0) AND isnotnull(deaths#26772)) THEN (cast(cases#26770 as double) / cast(deaths#26772 as double)) ELSE cast(null as double) END AS cases_per_death#26780]\n         +- Project [date#26768, county#26657, state#26658, fips#26659, cases#26770, deaths#26772, year#26774, month#26776, quarter(date#26768) AS quarter#26778]\n            +- Project [date#26768, county#26657, state#26658, fips#26659, cases#26770, deaths#26772, year#26774, month(date#26768) AS month#26776]\n               +- Project [date#26768, county#26657, state#26658, fips#26659, cases#26770, deaths#26772, year(date#26768) AS year#26774]\n                  +- Filter (cases#26770 > 0)\n                     +- Filter isnotnull(try_to_date(date#26768, Some(yyyy-MM-dd), Some(Etc/UTC), false))\n                        +- Project [date#26768, county#26657, state#26658, fips#26659, cases#26770, coalesce(try_cast(deaths#26661 as int), 0) AS deaths#26772]\n                           +- Project [date#26768, county#26657, state#26658, fips#26659, coalesce(try_cast(cases#26660 as int), 0) AS cases#26770, deaths#26661]\n                              +- Project [to_date(date#26656, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS date#26768, county#26657, state#26658, fips#26659, cases#26660, deaths#26661]\n                                 +- Relation [date#26656,county#26657,state#26658,fips#26659,cases#26660,deaths#26661] csv\n\n== Optimized Logical Plan ==\nProject [state#26658, year#26774, month#26776, total_cases#27428L, total_deaths#27429L, avg_daily_cases#27430, max_daily_cases#27431, num_records#27432L, cast(((cast(total_deaths#27429L as double) / cast(total_cases#27428L as double)) * 100.0) as decimal(10,2)) AS mortality_rate#27439]\n+- Aggregate [state#26658, year#26774, month#26776], [state#26658, year#26774, month#26776, sum(cases#26770) AS total_cases#27428L, sum(deaths#26772) AS total_deaths#27429L, (sum(cast(cases#26770 as double)) / cast(count(1) as double)) AS avg_daily_cases#27430, max(cases#26770) AS max_daily_cases#27431, count(1) AS num_records#27432L]\n   +- Project [state#26658, cases#26770, deaths#26772, year(date#26768) AS year#26774, month(date#26768) AS month#26776]\n      +- Project [cast(gettimestamp(date#26656, yyyy-MM-dd, TimestampType, try_to_date, Some(Etc/UTC), true) as date) AS date#26768, state#26658, coalesce(try_cast(cases#26660 as int), 0) AS cases#26770, coalesce(try_cast(deaths#26661 as int), 0) AS deaths#26772]\n         +- Filter ((coalesce(try_cast(cases#26660 as int), 0) > 0) AND isnotnull(cast(gettimestamp(cast(gettimestamp(date#26656, yyyy-MM-dd, TimestampType, try_to_date, Some(Etc/UTC), true) as date), yyyy-MM-dd, TimestampType, try_to_date, Some(Etc/UTC), false) as date)))\n            +- Relation [date#26656,county#26657,state#26658,fips#26659,cases#26660,deaths#26661] csv\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   ColumnarToRow\n   +- PhotonResultStage\n      +- PhotonProject [state#26658, year#26774, month#26776, total_cases#27428L, total_deaths#27429L, avg_daily_cases#27430, max_daily_cases#27431, num_records#27432L, cast(((cast(total_deaths#27429L as double) / cast(total_cases#27428L as double)) * 100.0) as decimal(10,2)) AS mortality_rate#27439]\n         +- PhotonGroupingAgg(limit=None, keys=[state#26658, year#26774, month#26776], functions=[finalmerge_sum(merge sum#27460L) AS sum(cases)#27434L, finalmerge_sum(merge sum#27699L) AS sum(deaths)#27435L, finalmerge_sum(merge sum#28104) AS sum(cases)#28101, finalmerge_count(merge count#28106L) AS count(1)#28102L, finalmerge_max(merge max#27701) AS max(cases)#27437], output=[state#26658, year#26774, month#26776, total_cases#27428L, total_deaths#27429L, avg_daily_cases#27430, max_daily_cases#27431, num_records#27432L])\n            +- PhotonShuffleExchangeSource\n               +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#29532]\n                  +- PhotonShuffleExchangeSink hashpartitioning(state#26658, year#26774, month#26776, 1024)\n                     +- PhotonGroupingAgg(limit=None, keys=[state#26658, year#26774, month#26776], functions=[partial_sum(cases#26770) AS sum#27460L, partial_sum(deaths#26772) AS sum#27699L, partial_sum(cast(cases#26770 as double)) AS sum#28104, partial_count(1) AS count#28106L, partial_max(cases#26770) AS max#27701], output=[state#26658, year#26774, month#26776, sum#27460L, sum#27699L, sum#28104, count#28106L, max#27701])\n                        +- PhotonProject [state#26658, cases#26770, deaths#26772, year(date#26768) AS year#26774, month(date#26768) AS month#26776]\n                           +- PhotonProject [cast(gettimestamp(date#26656, yyyy-MM-dd, TimestampType, try_to_date, Some(Etc/UTC), true) as date) AS date#26768, state#26658, coalesce(try_cast(cases#26660 as int), 0) AS cases#26770, coalesce(try_cast(deaths#26661 as int), 0) AS deaths#26772]\n                              +- PhotonFilter ((coalesce(try_cast(cases#26660 as int), 0) > 0) AND isnotnull(cast(gettimestamp(cast(gettimestamp(date#26656, yyyy-MM-dd, TimestampType, try_to_date, Some(Etc/UTC), true) as date), yyyy-MM-dd, TimestampType, try_to_date, Some(Etc/UTC), false) as date)))\n                                 +- PhotonRowToColumnar\n                                    +- FileScan csv [date#26656,state#26658,cases#26660,deaths#26661] Batched: false, DataFilters: [(coalesce(try_cast(cases#26660 as int), 0) > 0), isnotnull(cast(gettimestamp(cast(gettimestamp(d..., Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/COVID/covid-19-data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<date:string,state:string,cases:string,deaths:string>\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "# Show Execution Plan\n",
    "df_monthly_stats.explain(mode=\"extended\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab385973-b4dc-4979-9bc8-f1bfd8fe9a0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[9] Demonstrating Filter Pushdown:\n\nOptimized Query Plan (filters pushed down):\n== Physical Plan ==\n* ColumnarToRow (6)\n+- PhotonResultStage (5)\n   +- PhotonProject (4)\n      +- PhotonFilter (3)\n         +- PhotonRowToColumnar (2)\n            +- Scan csv  (1)\n\n\n(1) Scan csv \nOutput [4]: [date#26656, state#26658, cases#26660, deaths#26661]\nBatched: false\nLocation: InMemoryFileIndex [dbfs:/databricks-datasets/COVID/covid-19-data]\nPushedFilters: [IsNotNull(state), EqualTo(state,California)]\nReadSchema: struct<date:string,state:string,cases:string,deaths:string>\n\n(2) PhotonRowToColumnar\nInput [4]: [date#26656, state#26658, cases#26660, deaths#26661]\n\n(3) PhotonFilter\nInput [4]: [date#26656, state#26658, cases#26660, deaths#26661]\nArguments: ((((isnotnull(state#26658) AND (state#26658 = California)) AND (coalesce(try_cast(cases#26660 as int), 0) > 0)) AND (year(cast(gettimestamp(date#26656, yyyy-MM-dd, TimestampType, try_to_date, Some(Etc/UTC), true) as date)) = 2021)) AND isnotnull(cast(gettimestamp(cast(gettimestamp(date#26656, yyyy-MM-dd, TimestampType, try_to_date, Some(Etc/UTC), true) as date), yyyy-MM-dd, TimestampType, try_to_date, Some(Etc/UTC), false) as date)))\n\n(4) PhotonProject\nInput [4]: [date#26656, state#26658, cases#26660, deaths#26661]\nArguments: [cast(gettimestamp(date#26656, yyyy-MM-dd, TimestampType, try_to_date, Some(Etc/UTC), true) as date) AS date#26768, state#26658, coalesce(try_cast(cases#26660 as int), 0) AS cases#26770, coalesce(try_cast(deaths#26661 as int), 0) AS deaths#26772]\n\n(5) PhotonResultStage\nInput [4]: [date#26768, state#26658, cases#26770, deaths#26772]\n\n(6) ColumnarToRow [codegen id : 1]\nInput [4]: [date#26768, state#26658, cases#26770, deaths#26772]\n\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate Filter Pushdown\n",
    "print(\"\\n[9] Demonstrating Filter Pushdown:\")\n",
    "optimized_query = df_transformed \\\n",
    "    .filter(col(\"year\") == 2021) \\\n",
    "    .filter(col(\"state\") == \"California\") \\\n",
    "    .select(\"date\", \"state\", \"cases\", \"deaths\")\n",
    "\n",
    "print(\"\\nOptimized Query Plan (filters pushed down):\")\n",
    "optimized_query.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98dc1556-cc35-4f81-b8c5-2c2291998dd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Demonstrating Transformations (Lazy) vs Actions (Eager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9030785f-9e5c-4fbe-a848-18937013cf1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nApplying TRANSFORMATIONS (Lazy - not executed yet):\nTransformations defined (no computation performed)\nDataFrame type: <class 'pyspark.sql.connect.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Transformations (Lazy - no execution yet)\n",
    "print(\"\\nApplying TRANSFORMATIONS (Lazy - not executed yet):\")\n",
    "lazy_df = df_transformed \\\n",
    "    .filter(col(\"year\") == 2021) \\\n",
    "    .select(\"state\", \"date\", \"cases\") \\\n",
    "    .filter(col(\"cases\") > 5000)\n",
    "\n",
    "print(\"Transformations defined (no computation performed)\")\n",
    "print(f\"DataFrame type: {type(lazy_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9797374-cc6b-4dd7-92ee-6824b74cc8f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nExecuting ACTIONS (Eager - triggers computation):\n\nAction 1: count()\nCount: 85,962 (executed in 0.745s)\n\nAction 2: show()\n+-------+----------+-----+\n|  state|      date|cases|\n+-------+----------+-----+\n|Alabama|2021-01-01|13823|\n|Alabama|2021-01-01| 9584|\n|Alabama|2021-01-01| 7194|\n|Alabama|2021-01-01| 6978|\n|Alabama|2021-01-01| 6542|\n+-------+----------+-----+\nonly showing top 5 rows\nShow completed (executed in 0.679s)\n\nAction 3: collect()\nCollected 100 rows (executed in 0.493s)\n"
     ]
    }
   ],
   "source": [
    "# Actions (Eager - triggers execution)\n",
    "print(\"\\nExecuting ACTIONS (Eager - triggers computation):\")\n",
    "\n",
    "print(\"\\nAction 1: count()\")\n",
    "start = time.time()\n",
    "count = lazy_df.count()\n",
    "action_time1 = time.time() - start\n",
    "print(f\"Count: {count:,} (executed in {action_time1:.3f}s)\")\n",
    "\n",
    "print(\"\\nAction 2: show()\")\n",
    "start = time.time()\n",
    "lazy_df.show(5)\n",
    "action_time2 = time.time() - start\n",
    "print(f\"Show completed (executed in {action_time2:.3f}s)\")\n",
    "\n",
    "print(\"\\nAction 3: collect()\")\n",
    "start = time.time()\n",
    "collected = lazy_df.limit(100).collect()\n",
    "action_time3 = time.time() - start\n",
    "print(f\"Collected {len(collected)} rows (executed in {action_time3:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "397ad7e5-32e7-4e4e-bac1-6fd3543d11b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nDefining multiple transformations (chained operations):\n6 transformations defined in 0.67 milliseconds\nDataFrame object created: <class 'pyspark.sql.connect.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDefining multiple transformations (chained operations):\")\n",
    "start_lazy = time.time()\n",
    "\n",
    "# Chain multiple transformations - NO execution happens yet!\n",
    "lazy_df = df_transformed \\\n",
    "    .filter(col(\"year\") == 2021) \\\n",
    "    .filter(col(\"state\").isNotNull()) \\\n",
    "    .select(\"state\", \"date\", \"cases\", \"deaths\") \\\n",
    "    .filter(col(\"cases\") > 5000) \\\n",
    "    .withColumn(\"high_mortality\", col(\"deaths\") > 100) \\\n",
    "    .filter(col(\"high_mortality\") == True)\n",
    "\n",
    "lazy_time = time.time() - start_lazy\n",
    "\n",
    "print(f\"6 transformations defined in {lazy_time*1000:.2f} milliseconds\")\n",
    "print(f\"DataFrame object created: {type(lazy_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fff885b3-b12e-4a3a-86f6-db512c6dadba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Plan (Spark's optimization):\n== Physical Plan ==\n*(1) ColumnarToRow\n+- PhotonResultStage\n   +- PhotonProject [state#26658, date#26768, cases#26770, deaths#26772, (deaths#26772 > 100) AS high_mortality#28192]\n      +- PhotonProject [state#26658, cast(gettimestamp(date#26656, yyyy-MM-dd, TimestampType, try_to_date, Some(Etc/UTC), true) as date) AS date#26768, coalesce(try_cast(cases#26660 as int), 0) AS cases#26770, coalesce(try_cast(deaths#26661 as int), 0) AS deaths#26772]\n         +- PhotonFilter (((((isnotnull(state#26658) AND (coalesce(try_cast(cases#26660 as int), 0) > 0)) AND (coalesce(try_cast(cases#26660 as int), 0) > 5000)) AND (coalesce(try_cast(deaths#26661 as int), 0) > 100)) AND (year(cast(gettimestamp(date#26656, yyyy-MM-dd, TimestampType, try_to_date, Some(Etc/UTC), true) as date)) = 2021)) AND isnotnull(cast(gettimestamp(cast(gettimestamp(date#26656, yyyy-MM-dd, TimestampType, try_to_date, Some(Etc/UTC), true) as date), yyyy-MM-dd, TimestampType, try_to_date, Some(Etc/UTC), false) as date)))\n            +- PhotonRowToColumnar\n               +- FileScan csv [date#26656,state#26658,cases#26660,deaths#26661] Batched: false, DataFilters: [isnotnull(state#26658), (coalesce(try_cast(cases#26660 as int), 0) > 0), (coalesce(try_cast(case..., Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/COVID/covid-19-data], PartitionFilters: [], PushedFilters: [IsNotNull(state)], ReadSchema: struct<date:string,state:string,cases:string,deaths:string>\n\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "# Show the optimized execution plan\n",
    "print(\"Execution Plan (Spark's optimization):\")\n",
    "lazy_df.explain(mode=\"simple\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26090bca-e73e-495c-a26c-7dd1fe5135ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nExecuting ACTIONS (Eager - triggers computation):\nACTION 1: count() - counts all rows\nResult: 66,791 rows\nExecution time: 0.981 seconds\nNow data was actually processed!\n\nACTION 2: show() - displays sample rows\n+-------+----------+-----+------+--------------+\n|  state|      date|cases|deaths|high_mortality|\n+-------+----------+-----+------+--------------+\n|Alabama|2021-01-01|13823|   169|          true|\n|Alabama|2021-01-01| 9584|   157|          true|\n|Alabama|2021-01-01|53058|   719|          true|\n|Alabama|2021-01-01|22590|   179|          true|\n|Alabama|2021-01-01|26151|   508|          true|\n+-------+----------+-----+------+--------------+\nonly showing top 5 rows\nExecution time: 0.832 seconds\nNote: Slightly faster because Spark only needs 5 rows\n\nACTION 3: collect() - brings data to driver\nCollected 10 rows to driver memory\nExecution time: 0.478 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"\\nExecuting ACTIONS (Eager - triggers computation):\")\n",
    "\n",
    "# ACTION 1: count()\n",
    "print(\"ACTION 1: count() - counts all rows\")\n",
    "start = time.time()\n",
    "count = lazy_df.count()\n",
    "action_time1 = time.time() - start\n",
    "print(f\"Result: {count:,} rows\")\n",
    "print(f\"Execution time: {action_time1:.3f} seconds\")\n",
    "print(f\"Now data was actually processed!\\n\")\n",
    "\n",
    "# ACTION 2: show()\n",
    "print(\"ACTION 2: show() - displays sample rows\")\n",
    "start = time.time()\n",
    "lazy_df.show(5)\n",
    "action_time2 = time.time() - start\n",
    "print(f\"Execution time: {action_time2:.3f} seconds\")\n",
    "print(f\"Note: Slightly faster because Spark only needs 5 rows\\n\")\n",
    "\n",
    "# ACTION 3: collect()\n",
    "print(\"ACTION 3: collect() - brings data to driver\")\n",
    "start = time.time()\n",
    "collected = lazy_df.limit(10).collect()\n",
    "action_time3 = time.time() - start\n",
    "print(f\"Collected {len(collected)} rows to driver memory\")\n",
    "print(f\"Execution time: {action_time3:.3f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98fce049-664a-46d3-8b73-36b37a090294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nPerformance Comparison:\n\nOperation                                Time            Data Processed\n----------------------------------------------------------------------\nDefine all transformations (LAZY)          0.67 ms      None (0 bytes)\nExecute count() action                      981 ms      66,791 rows\nExecute show() action                       832 ms      5 rows (optimized)\nExecute collect() action                    478 ms      10 rows\n"
     ]
    }
   ],
   "source": [
    "#Compare the performance Lazy vs Eager:\n",
    "\n",
    "print(\"\\nPerformance Comparison:\\n\")\n",
    "print(f\"{'Operation':<40} {'Time':<15} {'Data Processed'}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Define all transformations (LAZY)':<40} {lazy_time*1000:>6.2f} ms      {'None (0 bytes)'}\")\n",
    "print(f\"{'Execute count() action':<40} {action_time1*1000:>6.0f} ms      {f'{count:,} rows'}\")\n",
    "print(f\"{'Execute show() action':<40} {action_time2*1000:>6.0f} ms      {'5 rows (optimized)'}\")\n",
    "print(f\"{'Execute collect() action':<40} {action_time3*1000:>6.0f} ms      {'10 rows'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d19acf57-d2ba-43a4-9625-883f81b6f82b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3835f6b-4161-4e95-b6dc-abbba29b857f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use a Unity Catalog volume (recommended) to save the parquet files: \n",
    "output_base = \"/Volumes/de/de/de/covid_pipeline_output\"\n",
    "\n",
    "output_path_parquet = f\"{output_base}/monthly_stats_parquet\"\n",
    "df_monthly_stats.write.mode(\"overwrite\").parquet(output_path_parquet)\n",
    "\n",
    "output_path_partitioned = f\"{output_base}/yearly_stats_partitioned\"\n",
    "df_yearly.write.mode(\"overwrite\").partitionBy(\"year\").parquet(output_path_partitioned)\n",
    "\n",
    "output_path_delta = f\"{output_base}/top_states_delta\"\n",
    "df_sql1.write.format(\"delta\").mode(\"overwrite\").save(output_path_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "018d4ae9-d45d-417c-8c34-f33d5b888552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nPipeline Execution Complete!\n\nData Processed:\n- Original Records: 1,227,256\n- Filtered Records: 1,224,611\n- Monthly Aggregations: 1,442\n- Yearly Aggregations: 220\n\nOptimizations Applied:\n✓ Early filtering to reduce data volume\n✓ Column pruning (selecting only needed columns)\n✓ Predicate pushdown demonstrated\n✓ Partitioned writes for efficient storage\n✓ Serverless compute automatic optimizations\n\nOutput Locations:\n- /Volumes/de/de/de/covid_pipeline_output/monthly_stats_parquet\n- /Volumes/de/de/de/covid_pipeline_output/yearly_stats_partitioned\n- /Volumes/de/de/de/covid_pipeline_output/top_states_delta\n\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Pipeline Execution Complete!\n",
    "\n",
    "Data Processed:\n",
    "- Original Records: {df_covid.count():,}\n",
    "- Filtered Records: {df_filtered.count():,}\n",
    "- Monthly Aggregations: {df_monthly_stats.count():,}\n",
    "- Yearly Aggregations: {df_yearly.count():,}\n",
    "\n",
    "Optimizations Applied:\n",
    "✓ Early filtering to reduce data volume\n",
    "✓ Column pruning (selecting only needed columns)\n",
    "✓ Predicate pushdown demonstrated\n",
    "✓ Partitioned writes for efficient storage\n",
    "✓ Serverless compute automatic optimizations\n",
    "\n",
    "Output Locations:\n",
    "- {output_path_parquet}\n",
    "- {output_path_partitioned}\n",
    "- {output_path_delta}\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98c7de1e-c3dd-4baf-94c4-57a31907b79f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DE_PySpark_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}